{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we set a standard random seed so that we can reproduce results\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we split the available training data into training, test and cross validation\n",
    "\n",
    "# Training data\n",
    "n_training_samples = 20000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "# Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype = np.int64))\n",
    "\n",
    "# Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the output size of a conv operation given the params\n",
    "\n",
    "def outputSize(in_size, kernel_size, stride, padding):\n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "    return(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(outputSize(30, 2, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SimpleCNN class that inherits pytorch.nn.module and implement a vanilla CNN\n",
    "\n",
    "class SimpleCNN (nn.Module):\n",
    "    \n",
    "    # batch shape for input: (3, 32, 32), 3 = RGB channels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # super().__init__()\n",
    "        \n",
    "        # input channels = 3, output channels = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # input channels = 16, output channels = 32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # 4608 input features, 64 output features \n",
    "        self.fc1 = nn.Linear(32*8*8, 128)\n",
    "        \n",
    "        # 64 input features, 10 output features for our defrined classes\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # compute the activation of the first convolution\n",
    "        # size changes from (3, 32, 32) to (18, 32, 32)\n",
    "        # print(\"Shape before 1st conv: \" + str(x.shape))\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # print(\"Shape after 1st conv: \" + str(x.shape))\n",
    "        # size changes from (18, 32, 32) to (18, 16, 16)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #print(\"Shape after 1st pool: \" + str(x.shape))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        #print(\"Shape after 2nd conv: \" + str(x.shape))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        #print(\"Shape after 2nd pool: \" + str(x.shape))\n",
    "        # Reshape data for the input layer of the net\n",
    "        # Size changes from (18, 16, 16) to (1, 4608)\n",
    "        # Recall that the -1 infers this dimension \n",
    "        \n",
    "        x = x.view(-1, 32*8*8)\n",
    "        \n",
    "        #print(\"Shape after view: \" + str(x.shape))\n",
    "        # computes the activation of the first fully connected layer\n",
    "        # size changes from (1,4608) to (1,64)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #x = leaky_relu(self.fc1(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"Shape after 1st fc1: \" + str(x.shape))\n",
    "        # Computes the second fully connected layer (activation applied later)\n",
    "        # Size changes from (1. 64) to (1, 10)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(\"Shape after fc2: \" + str(x.shape))\n",
    "        x = self.fc3(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLaoder\n",
    "# Takes in a dataset and a sampler for loading\n",
    "# num_workers deals with system memory and threads\n",
    "\n",
    "def get_train_loader(batch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch_size = number of images to be taken in a single batch\n",
    "    \"\"\"\n",
    "    train_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                  sampler=train_sampler, num_workers=2)\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Validation loaders have constant batch size, so we can define them directly\n",
    "\n",
    "test_loader = data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss and opitmizer functions\n",
    "# we use the CrossEntropyLoss and ADAM as Optimizer\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate = 0.001):\n",
    "    # Loss function\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the CNN \n",
    "\n",
    "def trainNet(net, batch_size, number_of_epochs, learning_rate):\n",
    "    # Print all the hyperparameters of the training iteration\n",
    "    print(\"Hyperparameters: \")\n",
    "    print(\"Batch size = \", batch_size)\n",
    "    print(\"epochs = \", number_of_epochs)\n",
    "    print(\"Learning Rate = \", learning_rate)\n",
    "    \n",
    "    # Get Training Data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    number_of_batches = len(train_loader)\n",
    "    \n",
    "    # Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    # Keep track of time\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # Loop for number_of_epochs\n",
    "    for epoch in range(number_of_epochs):\n",
    "        running_loss = 0.0\n",
    "        print_every = number_of_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Get inputs\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Wraps them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            # Set the parameter gradients to zero\n",
    "            # And make the forward pass, calculate gradient, do backprop\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss_size.data\n",
    "            total_train_loss += loss_size.data\n",
    "            \n",
    "            # Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t Train loss: {:.2f} took: {:.2f}s\".format(\n",
    "                epoch+1, int(100* (i+1)/number_of_batches),\n",
    "                running_loss / print_every,\n",
    "                time.time() - start_time))\n",
    "                \n",
    "                # Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "                \n",
    "        # At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            # Wrap tensors in variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data\n",
    "        \n",
    "        print(\"validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "    \n",
    "    print(\"Training finished. Took: {:.2f}s\".format(time.time() - training_start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "CUDA support: True\n",
      "Hyperparameters: \n",
      "Batch size =  8\n",
      "epochs =  20\n",
      "Learning Rate =  0.001\n",
      "Epoch 1, 10% \t Train loss: 2.09 took: 7.31s\n",
      "Epoch 1, 20% \t Train loss: 1.81 took: 2.82s\n",
      "Epoch 1, 30% \t Train loss: 1.62 took: 2.81s\n",
      "Epoch 1, 40% \t Train loss: 1.58 took: 2.82s\n",
      "Epoch 1, 50% \t Train loss: 1.51 took: 2.91s\n",
      "Epoch 1, 60% \t Train loss: 1.42 took: 2.85s\n",
      "Epoch 1, 70% \t Train loss: 1.43 took: 2.91s\n",
      "Epoch 1, 80% \t Train loss: 1.37 took: 2.92s\n",
      "Epoch 1, 90% \t Train loss: 1.35 took: 2.83s\n",
      "validation loss = 1.25\n",
      "Epoch 2, 10% \t Train loss: 1.24 took: 7.04s\n",
      "Epoch 2, 20% \t Train loss: 1.19 took: 2.80s\n",
      "Epoch 2, 30% \t Train loss: 1.21 took: 2.78s\n",
      "Epoch 2, 40% \t Train loss: 1.19 took: 2.79s\n",
      "Epoch 2, 50% \t Train loss: 1.16 took: 2.81s\n",
      "Epoch 2, 60% \t Train loss: 1.16 took: 2.76s\n",
      "Epoch 2, 70% \t Train loss: 1.15 took: 2.86s\n",
      "Epoch 2, 80% \t Train loss: 1.15 took: 2.86s\n",
      "Epoch 2, 90% \t Train loss: 1.12 took: 2.94s\n",
      "validation loss = 1.18\n",
      "Epoch 3, 10% \t Train loss: 0.98 took: 7.32s\n",
      "Epoch 3, 20% \t Train loss: 0.96 took: 2.87s\n",
      "Epoch 3, 30% \t Train loss: 1.02 took: 2.90s\n",
      "Epoch 3, 40% \t Train loss: 1.00 took: 2.86s\n",
      "Epoch 3, 50% \t Train loss: 1.03 took: 2.86s\n",
      "Epoch 3, 60% \t Train loss: 0.95 took: 2.87s\n",
      "Epoch 3, 70% \t Train loss: 0.99 took: 2.78s\n",
      "Epoch 3, 80% \t Train loss: 0.97 took: 2.82s\n",
      "Epoch 3, 90% \t Train loss: 0.99 took: 2.88s\n",
      "validation loss = 1.05\n",
      "Epoch 4, 10% \t Train loss: 0.82 took: 7.22s\n",
      "Epoch 4, 20% \t Train loss: 0.85 took: 2.87s\n",
      "Epoch 4, 30% \t Train loss: 0.84 took: 2.89s\n",
      "Epoch 4, 40% \t Train loss: 0.85 took: 2.80s\n",
      "Epoch 4, 50% \t Train loss: 0.85 took: 2.99s\n",
      "Epoch 4, 60% \t Train loss: 0.90 took: 2.90s\n",
      "Epoch 4, 70% \t Train loss: 0.85 took: 2.90s\n",
      "Epoch 4, 80% \t Train loss: 0.83 took: 2.98s\n",
      "Epoch 4, 90% \t Train loss: 0.86 took: 2.96s\n",
      "validation loss = 1.14\n",
      "Epoch 5, 10% \t Train loss: 0.75 took: 7.04s\n",
      "Epoch 5, 20% \t Train loss: 0.71 took: 2.77s\n",
      "Epoch 5, 30% \t Train loss: 0.68 took: 2.89s\n",
      "Epoch 5, 40% \t Train loss: 0.73 took: 2.97s\n",
      "Epoch 5, 50% \t Train loss: 0.74 took: 2.92s\n",
      "Epoch 5, 60% \t Train loss: 0.75 took: 2.77s\n",
      "Epoch 5, 70% \t Train loss: 0.72 took: 2.81s\n",
      "Epoch 5, 80% \t Train loss: 0.77 took: 2.80s\n",
      "Epoch 5, 90% \t Train loss: 0.76 took: 2.80s\n",
      "validation loss = 1.10\n",
      "Epoch 6, 10% \t Train loss: 0.59 took: 7.17s\n",
      "Epoch 6, 20% \t Train loss: 0.59 took: 2.98s\n",
      "Epoch 6, 30% \t Train loss: 0.59 took: 2.86s\n",
      "Epoch 6, 40% \t Train loss: 0.63 took: 2.86s\n",
      "Epoch 6, 50% \t Train loss: 0.61 took: 2.90s\n",
      "Epoch 6, 60% \t Train loss: 0.61 took: 3.04s\n",
      "Epoch 6, 70% \t Train loss: 0.63 took: 3.00s\n",
      "Epoch 6, 80% \t Train loss: 0.66 took: 3.05s\n",
      "Epoch 6, 90% \t Train loss: 0.68 took: 2.98s\n",
      "validation loss = 1.10\n",
      "Epoch 7, 10% \t Train loss: 0.49 took: 7.11s\n",
      "Epoch 7, 20% \t Train loss: 0.48 took: 2.97s\n",
      "Epoch 7, 30% \t Train loss: 0.48 took: 2.79s\n",
      "Epoch 7, 40% \t Train loss: 0.52 took: 2.84s\n",
      "Epoch 7, 50% \t Train loss: 0.52 took: 2.92s\n",
      "Epoch 7, 60% \t Train loss: 0.54 took: 2.79s\n",
      "Epoch 7, 70% \t Train loss: 0.57 took: 2.82s\n",
      "Epoch 7, 80% \t Train loss: 0.54 took: 2.82s\n",
      "Epoch 7, 90% \t Train loss: 0.59 took: 2.84s\n",
      "validation loss = 1.23\n",
      "Epoch 8, 10% \t Train loss: 0.34 took: 7.10s\n",
      "Epoch 8, 20% \t Train loss: 0.42 took: 2.81s\n",
      "Epoch 8, 30% \t Train loss: 0.42 took: 2.82s\n",
      "Epoch 8, 40% \t Train loss: 0.47 took: 2.82s\n",
      "Epoch 8, 50% \t Train loss: 0.48 took: 2.83s\n",
      "Epoch 8, 60% \t Train loss: 0.47 took: 2.84s\n",
      "Epoch 8, 70% \t Train loss: 0.44 took: 2.80s\n",
      "Epoch 8, 80% \t Train loss: 0.46 took: 2.86s\n",
      "Epoch 8, 90% \t Train loss: 0.46 took: 2.85s\n",
      "validation loss = 1.33\n",
      "Epoch 9, 10% \t Train loss: 0.31 took: 7.25s\n",
      "Epoch 9, 20% \t Train loss: 0.34 took: 2.96s\n",
      "Epoch 9, 30% \t Train loss: 0.32 took: 2.93s\n",
      "Epoch 9, 40% \t Train loss: 0.40 took: 2.89s\n",
      "Epoch 9, 50% \t Train loss: 0.40 took: 2.87s\n",
      "Epoch 9, 60% \t Train loss: 0.37 took: 2.82s\n",
      "Epoch 9, 70% \t Train loss: 0.35 took: 2.78s\n",
      "Epoch 9, 80% \t Train loss: 0.43 took: 2.80s\n",
      "Epoch 9, 90% \t Train loss: 0.39 took: 2.81s\n",
      "validation loss = 1.49\n",
      "Epoch 10, 10% \t Train loss: 0.27 took: 7.06s\n",
      "Epoch 10, 20% \t Train loss: 0.28 took: 2.78s\n",
      "Epoch 10, 30% \t Train loss: 0.26 took: 2.84s\n",
      "Epoch 10, 40% \t Train loss: 0.30 took: 2.91s\n",
      "Epoch 10, 50% \t Train loss: 0.33 took: 2.88s\n",
      "Epoch 10, 60% \t Train loss: 0.33 took: 2.89s\n",
      "Epoch 10, 70% \t Train loss: 0.31 took: 2.95s\n",
      "Epoch 10, 80% \t Train loss: 0.35 took: 2.85s\n",
      "Epoch 10, 90% \t Train loss: 0.31 took: 2.83s\n",
      "validation loss = 1.65\n",
      "Epoch 11, 10% \t Train loss: 0.21 took: 7.04s\n",
      "Epoch 11, 20% \t Train loss: 0.26 took: 2.80s\n",
      "Epoch 11, 30% \t Train loss: 0.22 took: 2.81s\n",
      "Epoch 11, 40% \t Train loss: 0.26 took: 2.81s\n",
      "Epoch 11, 50% \t Train loss: 0.27 took: 2.80s\n",
      "Epoch 11, 60% \t Train loss: 0.30 took: 2.80s\n",
      "Epoch 11, 70% \t Train loss: 0.27 took: 2.82s\n",
      "Epoch 11, 80% \t Train loss: 0.30 took: 2.80s\n",
      "Epoch 11, 90% \t Train loss: 0.30 took: 2.79s\n",
      "validation loss = 1.77\n",
      "Epoch 12, 10% \t Train loss: 0.17 took: 7.10s\n",
      "Epoch 12, 20% \t Train loss: 0.17 took: 2.96s\n",
      "Epoch 12, 30% \t Train loss: 0.22 took: 2.84s\n",
      "Epoch 12, 40% \t Train loss: 0.19 took: 2.82s\n",
      "Epoch 12, 50% \t Train loss: 0.25 took: 2.82s\n",
      "Epoch 12, 60% \t Train loss: 0.25 took: 3.01s\n",
      "Epoch 12, 70% \t Train loss: 0.22 took: 2.91s\n",
      "Epoch 12, 80% \t Train loss: 0.27 took: 3.09s\n",
      "Epoch 12, 90% \t Train loss: 0.25 took: 3.04s\n",
      "validation loss = 1.91\n",
      "Epoch 13, 10% \t Train loss: 0.17 took: 7.29s\n",
      "Epoch 13, 20% \t Train loss: 0.15 took: 2.93s\n",
      "Epoch 13, 30% \t Train loss: 0.16 took: 3.05s\n",
      "Epoch 13, 40% \t Train loss: 0.22 took: 2.98s\n",
      "Epoch 13, 50% \t Train loss: 0.20 took: 2.97s\n",
      "Epoch 13, 60% \t Train loss: 0.20 took: 2.86s\n",
      "Epoch 13, 70% \t Train loss: 0.23 took: 2.92s\n"
     ]
    }
   ],
   "source": [
    "# Create the network and run it\n",
    "\n",
    "cnn = SimpleCNN()\n",
    "cnn = cnn.to(device)\n",
    "print(cnn)\n",
    "train_on_gpu = torch.cuda.is_available() #will return true if gpu available\n",
    "print(\"CUDA support: \" + str(train_on_gpu))\n",
    "# move tensors to GPU if CUDA is available\n",
    "trainNet(cnn, batch_size=8, number_of_epochs=20, learning_rate=0.001)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
